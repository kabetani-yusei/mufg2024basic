{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>purposeの欠損値を'purpose_non'で埋める"
      ],
      "metadata": {
        "id": "Xo-BxN5WJvNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 準備"
      ],
      "metadata": {
        "id": "tkNZN2EwJ2kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google DriveのマウントとSEED値とPathの設定"
      ],
      "metadata": {
        "id": "PKWHA5HzKK9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 2681\n",
        "K_FOLD_SEED = 720\n",
        "NOTEBOOK = \"欠損値あり4\"\n",
        "INPUT_PATH = \"/content/drive/MyDrive/input/mufg2024/\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/model_save/\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tn9WYJggKKIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9659d60-49f8-4f11-a0d2-550b822b4418"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ライブラリの読み込み"
      ],
      "metadata": {
        "id": "PLXryWhsLEHA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2ZRGNSijJoit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2a88ee-b526-4ccb-ccf6-b2b8eff349c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.1.4)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n",
            "Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install category_encoders\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import category_encoders as ce\n",
        "import itertools\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "\n",
        "warnings.simplefilter('ignore')  # 不要な警告を表示しない"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 前処理"
      ],
      "metadata": {
        "id": "MZmjtgIiLVs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データの読み込み"
      ],
      "metadata": {
        "id": "MxXAtPUxLXYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(INPUT_PATH + \"train.csv\", index_col=0)\n",
        "test = pd.read_csv(INPUT_PATH + \"test.csv\", index_col=0)"
      ],
      "metadata": {
        "id": "wjCUU-NQLZMJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 前処理・特徴量生成"
      ],
      "metadata": {
        "id": "0QQaSKIXLeMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 関数"
      ],
      "metadata": {
        "id": "1WVnB9YyZVSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 特徴量生成\n",
        "def add_new_feature(dataset):\n",
        "    # logを取った特徴量の追加\n",
        "    dataset['annual.inc.log'] = np.log1p(dataset['annual.inc'])\n",
        "    dataset['revol.bal.log'] = np.log1p(dataset['revol.bal'])\n",
        "\n",
        "    # 時間データに対する処理\n",
        "    dataset['days.with.cr.line_6month'] = dataset['days.with.cr.line'] // 182\n",
        "    dataset['days.with.cr.line_1year'] = dataset['days.with.cr.line'] // 365\n",
        "    dataset['days.with.cr.line_2year'] = dataset['days.with.cr.line'] // 730\n",
        "\n",
        "\n",
        "# 特徴量をまとめたときの他の特徴量の性質を調べる\n",
        "def aggregated_features(cat_col, agg_col, train, test):\n",
        "    if cat_col == agg_col:\n",
        "        return\n",
        "    agg_types = ['max', 'min', 'mean', 'std', 'median']\n",
        "    quantiles = [0.25, 0.75]\n",
        "\n",
        "    for agg_type in agg_types:\n",
        "        new_col_name = f\"{cat_col}_{agg_col}_{agg_type}\"\n",
        "        temp = pd.concat([train[[cat_col, agg_col]], test[[cat_col, agg_col]]])\n",
        "        temp = temp.groupby(cat_col)[agg_col].agg([agg_type]).reset_index().rename(columns={agg_type: new_col_name})\n",
        "        temp.index = list(temp[cat_col])\n",
        "        temp = temp[new_col_name].to_dict()\n",
        "\n",
        "        # trainとtestの新しい特徴量を作成\n",
        "        train[new_col_name] = train[cat_col].map(temp)\n",
        "        test[new_col_name] = test[cat_col].map(temp)\n",
        "\n",
        "    # max - min の集計\n",
        "    max_col_name = f\"{cat_col}_{agg_col}_max\"\n",
        "    min_col_name = f\"{cat_col}_{agg_col}_min\"\n",
        "    max_min_col_name = f\"{cat_col}_{agg_col}_max_min\"\n",
        "\n",
        "    temp = pd.concat([train[[cat_col, agg_col]], test[[cat_col, agg_col]]])\n",
        "    max_min = temp.groupby(cat_col)[agg_col].agg(['max', 'min']).reset_index()\n",
        "    max_min[max_min_col_name] = max_min['max'] - max_min['min']\n",
        "    max_min = max_min[[cat_col, max_min_col_name]].set_index(cat_col).to_dict()[max_min_col_name]\n",
        "\n",
        "    train[max_min_col_name] = train[cat_col].map(max_min)\n",
        "    test[max_min_col_name] = test[cat_col].map(max_min)\n",
        "\n",
        "    # quantile の集計\n",
        "    for q in quantiles:\n",
        "        quantile_col_name = f\"{cat_col}_{agg_col}_quantile_{int(q*100)}\"\n",
        "        temp = pd.concat([train[[cat_col, agg_col]], test[[cat_col, agg_col]]])\n",
        "        quantile_values = temp.groupby(cat_col)[agg_col].quantile(q).reset_index().rename(columns={agg_col: quantile_col_name})\n",
        "        quantile_values.index = list(quantile_values[cat_col])\n",
        "        quantile_dict = quantile_values[quantile_col_name].to_dict()\n",
        "\n",
        "        train[quantile_col_name] = train[cat_col].map(quantile_dict)\n",
        "        test[quantile_col_name] = test[cat_col].map(quantile_dict)\n",
        "\n",
        "\n",
        "\n",
        "# Target Encoding\n",
        "def _target_encoding(train, test, column, target):\n",
        "    target_encoder = ce.TargetEncoder(cols=[column])\n",
        "    train_encoded = target_encoder.fit_transform(train[column], train[target])\n",
        "    test_encoded = target_encoder.transform(test[column])\n",
        "    return train_encoded, test_encoded\n",
        "def target_encoding(target_encoding_columns, train, test):\n",
        "    target_column = \"not.fully.paid\"\n",
        "    for column in target_encoding_columns:\n",
        "        # Target Encodingを実行\n",
        "        train_encoded, test_encoded = _target_encoding(train, test, column, target_column)\n",
        "        train.drop(column, axis=1, inplace=True)\n",
        "        train[column] = train_encoded\n",
        "        test.drop(column, axis=1, inplace=True)\n",
        "        test[column] = test_encoded\n",
        "\n",
        "\n",
        "# 四則演算を適用する全ての特徴量のペアを生成\n",
        "def make_new_features(df, del_list, ng_list, time_list):\n",
        "  feature_names = [col for col in df.columns if col not in del_list]\n",
        "  for i in range(len(feature_names)):\n",
        "      for j in range(i, len(feature_names)):\n",
        "          if feature_names[i] in ng_list and feature_names[j] in ng_list:continue\n",
        "          if feature_names[i] in time_list and feature_names[j] in time_list:continue\n",
        "          f1 = feature_names[i]\n",
        "          f2 = feature_names[j]\n",
        "          if f1 == f2:\n",
        "            df[f'{f1}_times_{f2}'] = df[f1] * df[f2]\n",
        "            continue\n",
        "          df[f'{f1}_times_{f2}'] = df[f1] * df[f2]\n",
        "          df[f'{f1}_divided_by_{f2}'] = np.where(df[f2] != 0, df[f1] / df[f2], 0)\n"
      ],
      "metadata": {
        "id": "7jLpYW9UNoyb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 処理"
      ],
      "metadata": {
        "id": "cgOebBIlZZII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 欠損値と外れ値除去をする\n",
        "train.dropna(subset=['revol.util', 'revol.bal', 'installment'], inplace=True)\n",
        "train = train[train['int.rate'] < 0.5]\n",
        "\n",
        "# 特徴量生成\n",
        "add_new_feature(train)\n",
        "add_new_feature(test)\n",
        "\n",
        "# 特徴量を区切った時の別の特徴量のmin, maxなどの情報を新たな特徴量として追加する\n",
        "time_features = [\n",
        "    'days.with.cr.line_6month',\n",
        "    'days.with.cr.line_1year',\n",
        "    'days.with.cr.line_2year',\n",
        "]\n",
        "category_list = [\"purpose\", \"delinq.2yrs\", \"inq.last.6mths\", \"pub.rec\"] + time_features\n",
        "non_category_list = [\"int.rate\", \"fico\", \"inq.last.6mths\", \"installment\"]\n",
        "original_list = train.columns.tolist()\n",
        "for cat_col in category_list:\n",
        "    for agg_col in non_category_list:\n",
        "        aggregated_features(cat_col, agg_col, train, test)\n",
        "del_list = [col for col in test.columns if col not in original_list]\n",
        "ng_list = ['inq.last.6mths', 'delinq.2yrs', 'pub.rec']\n",
        "\n",
        "# Target Encoding\n",
        "target_encoding_columns = [\"purpose\"]\n",
        "target_encoding(target_encoding_columns, train, test)\n",
        "\n",
        "# 前処理後のデータ\n",
        "df_train = train.drop(columns=['not.fully.paid'])\n",
        "df_train = df_train.astype(float)\n",
        "df_test = test.astype(float)\n",
        "y = train['not.fully.paid']\n",
        "\n",
        "# 特徴量生成(四則演算)\n",
        "make_new_features(df_train, del_list, ng_list, time_features)\n",
        "make_new_features(df_test, del_list, ng_list, time_features)\n",
        "\n",
        "# 処理ができているかの確認をする\n",
        "print(df_train.shape)\n",
        "print(y.shape)\n",
        "print(df_test.shape)"
      ],
      "metadata": {
        "id": "h7jdqmaaLsFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "679840f3-3931-435f-db87-2420bb1e7e85"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40761, 504)\n",
            "(40761,)\n",
            "(40786, 504)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 学習"
      ],
      "metadata": {
        "id": "If6-IJ7EfkQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = ['int.rate', 'fico', 'inq.last.6mths', 'delinq.2yrs', 'pub.rec', 'days.with.cr.line_6month', 'revol.util', 'days.with.cr.line_1year', 'revol.bal', 'installment', 'dti', 'annual.inc', 'purpose', 'days.with.cr.line_1year_fico_mean', 'purpose_fico_quantile_75', 'days.with.cr.line_2year_int.rate_max', 'delinq.2yrs_installment_mean', 'pub.rec_fico_quantile_25', 'days.with.cr.line_2year_inq.last.6mths_quantile_25', 'inq.last.6mths_installment_mean', 'purpose_installment_quantile_25', 'inq.last.6mths_installment_quantile_25', 'inq.last.6mths_fico_quantile_75', 'inq.last.6mths_fico_quantile_25']\n",
        "print(len(selected_features))\n",
        "print(selected_features)\n",
        "# 選択された特徴量でのデータセットの準備\n",
        "df_train_selected = df_train[selected_features]\n",
        "df_test_selected = df_test[selected_features]\n",
        "y = y.reset_index(drop=True).values if isinstance(y, pd.Series) else y\n",
        "\n",
        "# 説明変数と目的変数に分ける\n",
        "X = df_train_selected.copy()\n",
        "y = y.copy()\n",
        "X_test = df_test_selected.copy()"
      ],
      "metadata": {
        "id": "QuFTXfMGZ8Tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3fe7c27-7dba-4054-a99e-90672daba1f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "['int.rate', 'fico', 'inq.last.6mths', 'delinq.2yrs', 'pub.rec', 'days.with.cr.line_6month', 'revol.util', 'days.with.cr.line_1year', 'revol.bal', 'installment', 'dti', 'annual.inc', 'purpose', 'days.with.cr.line_1year_fico_mean', 'purpose_fico_quantile_75', 'days.with.cr.line_2year_int.rate_max', 'delinq.2yrs_installment_mean', 'pub.rec_fico_quantile_25', 'days.with.cr.line_2year_inq.last.6mths_quantile_25', 'inq.last.6mths_installment_mean', 'purpose_installment_quantile_25', 'inq.last.6mths_installment_quantile_25', 'inq.last.6mths_fico_quantile_75', 'inq.last.6mths_fico_quantile_25']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "\n",
        "# モデルを読み込む関数\n",
        "def load_model(idx):\n",
        "    load_path = NOTEBOOK + f\"_{idx}.pkl\"\n",
        "    with open(load_path, 'rb') as file:\n",
        "        model = pickle.load(file)\n",
        "    return model\n",
        "\n",
        "# 各FoldのAUCスコアを格納\n",
        "auc_scores = []\n",
        "y_train_pred = np.zeros(X.shape[0])\n",
        "y_test_pred = np.zeros(X_test.shape[0])\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=K_FOLD_SEED)\n",
        "\n",
        "# 各Foldに対してLightGBMモデルを訓練\n",
        "for i, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n",
        "    X_train, X_valid = X.iloc[tr_idx], X.iloc[va_idx]\n",
        "    y_train, y_valid = y[tr_idx], y[va_idx]\n",
        "    model = load_model(i)\n",
        "    val_pred = model.predict_proba(X_valid, num_iteration=model.best_iteration_)[:, 1]\n",
        "    auc = roc_auc_score(y_valid, val_pred)\n",
        "    auc_scores.append(auc)\n",
        "    print(f\"Fold {i+1} のAUCスコア: {auc}\")\n",
        "\n",
        "    y_test_pred += model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1] / kf.n_splits\n",
        "    y_train_pred[va_idx] = val_pred\n",
        "\n",
        "# 各FoldのAUCスコアの平均を求める\n",
        "cv_score = np.mean(auc_scores)\n",
        "print(f'Cross-Validation AUC Score: {cv_score}')"
      ],
      "metadata": {
        "id": "d-BgFP8RgXFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2733bfcb-4f14-48af-f2d6-74bf0249bbfa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 のAUCスコア: 0.7959857582904342\n",
            "Fold 2 のAUCスコア: 0.7969197381352158\n",
            "Fold 3 のAUCスコア: 0.7974730394138372\n",
            "Fold 4 のAUCスコア: 0.7939639690581213\n",
            "Fold 5 のAUCスコア: 0.798773535441038\n",
            "Cross-Validation AUC Score: 0.7966232080677292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# X_trainの予測値とX_testの予測値をcsv形式で保存する"
      ],
      "metadata": {
        "id": "7HCAUWgiBcTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = NOTEBOOK + \"_train.csv\"\n",
        "test_path = NOTEBOOK + \"_test.csv\"\n",
        "\n",
        "# df_test_predをcsv形式にする\n",
        "submit = pd.read_csv(INPUT_PATH + \"sample_submission.csv\", header=None)\n",
        "submit[1] = y_test_pred\n",
        "submit.to_csv(test_path, header=None, index=False)\n",
        "\n",
        "#df_train_predをcsv形式にする\n",
        "df_train_pred = pd.DataFrame({'prediction': y_train_pred})\n",
        "df_train_pred.to_csv(train_path, header=None, index=True)"
      ],
      "metadata": {
        "id": "rtiDjUpZgcmc"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}